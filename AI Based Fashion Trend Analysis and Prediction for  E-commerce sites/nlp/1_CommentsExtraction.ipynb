{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:    \n",
    "        import re\n",
    "        import time\n",
    "        import requests\n",
    "        import pandas as pd\n",
    "        from bs4 import BeautifulSoup\n",
    "        filename = \"updated_blikkin_flipkart\"\n",
    "        data = pd.read_csv(filename+'.csv',index_col=0)\n",
    "        print(\"data shape:\",data.shape)\n",
    "        # cols = ['total']\n",
    "        total = data['total']\n",
    "        data_url = []\n",
    "        data_code = []\n",
    "        for string in total:\n",
    "            url_match = re.search(r\"'(https://www\\.flipkart\\.com/[^']+)'\", string)\n",
    "            url = url_match.group(1) if url_match else None\n",
    "            data_url.append(url)\n",
    "            words = string.split(\"}\")[1].split(\", '\")[1].split(\"', [\")[0]\n",
    "            data_code.append(words)\n",
    "        data_dict = dict(zip(data_code,data_url))\n",
    "        review_pages_dictionary = dict()\n",
    "        for dc,dl in data_dict.items():\n",
    "            if dl!=None:\n",
    "                review_pages_list = []\n",
    "                review_pages_list.append(dl)\n",
    "                i=2\n",
    "                while i<=10:\n",
    "                    nextLink = dl+f\"&page={i}\"\n",
    "                    #print(nextLink)\n",
    "                    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "                    response = requests.get(nextLink, headers=headers)\n",
    "                    #print(response,response.ok)\n",
    "                    if response.ok:\n",
    "                        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                        review = html_soup.find_all('div', {\"class\":\"col _2wzgFH K0kLPL _1QgsS5\"})\n",
    "                        if review:\n",
    "                            review_pages_list.append(nextLink)               \n",
    "                    i=i+1\n",
    "            review_pages_dictionary[dc]=review_pages_list\n",
    "        codeList=[]\n",
    "        rating=[]\n",
    "        reviews=[]\n",
    "        name=[]\n",
    "        dtime=[]\n",
    "        ifCertified=[]\n",
    "        for dc,dl in review_pages_dictionary.items():\n",
    "            if dl!=None:\n",
    "                for l in dl:\n",
    "                    if l!=None:\n",
    "                        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "                        response = requests.get(l, headers=headers)\n",
    "                        if response.ok:\n",
    "                            html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                            review = html_soup.find_all('div', {\"class\":\"col _2wzgFH K0kLPL _1QgsS5\"})\n",
    "                            if review:\n",
    "                                for rev in review:\n",
    "                                    codeList.append(dc)\n",
    "                                    rat=rev.find(\"div\",{\"class\":\"_3LWZlK _1BLPMq _3B8WaH\"})\n",
    "                                    rating.append(rat.get_text() if rat else 0)\n",
    "                                    revw=rev.find(\"div\",{\"class\":\"_6K-7Co\"})\n",
    "                                    reviews.append(revw.get_text() if revw else '=')\n",
    "                                    n=rev.find(\"p\",{\"class\":\"_2sc7ZR _2V5EHH _1QgsS5\"})\n",
    "                                    name.append(n.get_text() if n else \"=\")\n",
    "                                    my=rev.find(\"div\",{\"class\":\"row _1ExUpQ\"}).find_all(\"p\",{\"class\":\"_2sc7ZR\"})[1].get_text()\n",
    "                                    dtime.append(my if my else 0)\n",
    "                                    ic=rev.find(\"p\",{\"class\":\"_2mcZGG\"})\n",
    "                                    ifCertified.append(ic.get_text() if ic else \"-\")\n",
    "                        else:\n",
    "                            print(\"Timeout !!!!\")\n",
    "        final_dataframe = pd.DataFrame({\n",
    "            'Code': codeList,\n",
    "            'Rating': rating,\n",
    "            'Review': reviews,\n",
    "            'Name': name,\n",
    "            'Time': dtime,\n",
    "            'Certified': ifCertified\n",
    "        })\n",
    "        final_dataframe.to_csv(\"Reviews_ALL_\"+filename+\".csv\", index=False)\n",
    "    except Exception as e:\n",
    "        print(\"\\nError !!!!!!!\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c58e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedd0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
